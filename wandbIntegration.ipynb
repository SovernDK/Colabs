{"cells":[{"cell_type":"markdown","metadata":{"id":"CJrzD87BCmqR"},"source":["# **Preparation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3QWhqqE5C7y"},"outputs":[],"source":["!pip install datasets sentencepiece accelerate bitsandbytes evaluate xformers deepspeed\n","!pip install git+https://github.com/huggingface/transformers.git@main"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9z4k26_35NcT"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"TytTtFkJCU8t"},"source":["# **Fine tuning**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-Ls8XVCfO_c"},"outputs":[],"source":["!pip install wandb\n","\n","import wandb\n","wandb.login()\n","wandb.init(project=\"opisy\")\n","\n","%env WANDB_PROJECT=develop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PAvRYy5h1wJG"},"outputs":[],"source":["import wandb\n","\n","wandb.login()\n","run = wandb.init(project=\"opisy\")\n","artifact = run.use_artifact('sovern-development/opisy/descriptions:latest', type='dataset')\n","artifact_dir = artifact.download()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v-K5sk6Y5TR4"},"outputs":[],"source":["import torch\n","import transformers\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from transformers import DataCollatorWithPadding\n","from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments, AutoTokenizer, AutoConfig, AutoModelForCausalLM\n","from datasets import load_dataset"]},{"cell_type":"markdown","metadata":{"id":"ub48KNHVC0BU"},"source":["**Load model and tokenizer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cF2aXIPu5e6u"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","base_model = \"sdadas/polish-gpt2-small\"  #Use small (176M params) model for testing on colab\n","\n","model = AutoModelForCausalLM.from_pretrained(base_model, device_map='auto', use_cache=False)\n","model.to(device)\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_model)\n","if tokenizer.pad_token_id is None:\n","  tokenizer.pad_token_id = tokenizer.eos_token_id"]},{"cell_type":"markdown","metadata":{"id":"kOnbxQa1Csm8"},"source":["**Pre-process data**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QdVvqeJ656_"},"outputs":[],"source":["def generate(data_point):\n","    return {\n","         'text': [\n","            f\"Produkt:\\n{name}, {category}\\n\\n###\\n\\n{description}[END]\"\n","            for name, category, description in zip(data_point['name'], data_point['category'], data_point['description'])\n","        ]\n","    }\n","\n","def tokenize(examples):\n","    return tokenizer(examples['text'], truncation=True, padding=\"max_length\", max_length=2048)\n","\n","dataset = load_dataset(\"csv\", data_files='/content/descriptions.csv')\n","dataset = dataset.map(generate, batched=True)\n","\n","tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=['name', 'category', 'description', 'code', 'brand', 'text'])\n","tokenized_dataset = tokenized_dataset.with_format(\"torch\")\n","split_datasets = tokenized_dataset['train'].train_test_split(test_size=0.1, seed=42, shuffle=True)\n","\n","data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n","# train_dataloader = DataLoader(\n","#     split_datasets[\"train\"], batch_size=16, shuffle=True, collate_fn=data_collator\n","# )\n","# test_dataloader = DataLoader(\n","#     split_datasets[\"test\"], batch_size=16, shuffle=True, collate_fn=data_collator\n","# )\n"]},{"cell_type":"markdown","metadata":{"id":"uLfbLlRzC4lA"},"source":["**Finetuning**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1fdmgAp52qP"},"outputs":[],"source":["import os\n","import transformers\n","from transformers import TrainingArguments\n","from datasets import load_metric\n","\n","output_dir = \"/content/portia-ai\"\n","\n","training_args = TrainingArguments(\n","      #paths\n","      output_dir=output_dir,\n","      overwrite_output_dir=True,\n","      #optimization\n","      per_device_train_batch_size=4,\n","      gradient_accumulation_steps=16,\n","      gradient_checkpointing=True,\n","      #training params\n","      num_train_epochs=3,\n","      learning_rate=5e-3,\n","      optim=\"adamw_torch\",\n","      fp16=True,\n","      warmup_steps=400,\n","      #smart batching\n","      group_by_length=True,\n","      #eval\n","      # load_best_model_at_end=True,\n","      evaluation_strategy=\"steps\",\n","      eval_steps=200,\n","      # save_strategy=\"steps\",\n","      # save_steps=1000,\n","      #wandb\n","      report_to=\"wandb\",\n","      run_name=\"develop\",\n",")\n","\n","trainer = transformers.Trainer(model=model,\n","            train_dataset=split_datasets['train'],\n","            eval_dataset=split_datasets['test'],\n","            args=training_args,\n","            data_collator=data_collator\n",")\n","\n","result = trainer.train(resume_from_checkpoint=False)\n","model.save_pretrained(output_dir)\n","\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCturAVPhQaI"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"3Qn3wC0-CRte"},"source":["# **Inference**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-IipDrmjCRTw"},"outputs":[],"source":["model_path = \"/content/portia-ai\"\n","\n","def generate_text(sequence, max_length):\n","    inference_model = AutoModelForCausalLM.from_pretrained(model_path)\n","    inference_model.to(device)\n","    inference_model.eval()\n","\n","    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n","    # if device is \"cuda\":\n","    ids = ids.cuda()\n","\n","    final_outputs = inference_model.generate(\n","        ids,\n","        do_sample=True,\n","        max_length=max_length,\n","        pad_token_id=inference_model.config.eos_token_id,\n","        top_k=50,\n","        top_p=0.95,\n","        repetition_penalty=1.2\n","    )\n","    print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))\n","\n","generate_text(\"Napisz reklamÄ™ jednego produktu.\\n\\n<Nazwa>\\nLaptop Acer\\n\\n<Reklama>\\n\", 2048)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPacRCWaILu3H0Kt7vfNcnW","gpuType":"T4","machine_shape":"hm","mount_file_id":"1MySC70qIAFEF4tySwEYeyF9r7HjnJFLX","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
