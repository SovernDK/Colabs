{"cells":[{"cell_type":"markdown","metadata":{"id":"BZPHEMPQPPLo"},"source":["Check nvidia config on machine"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N-Pm7qhh7MdO"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"C_OIAnlFO7ul"},"source":["Download needed libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VMKrkpU5F2Z"},"outputs":[],"source":["!pip3 install transformers datasets sentencepiece accelerate bitsandbytes fire peft"]},{"cell_type":"markdown","metadata":{"id":"ovyyB8OQPT_s"},"source":["Mount gdrive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4389,"status":"ok","timestamp":1682172609629,"user":{"displayName":"Typowy Geek","userId":"09368889698903750967"},"user_tz":-120},"id":"MkGJmdsRcaKu","outputId":"babf8448-498f-4925-fea4-854e06651578"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"6rklcG30PYKn"},"source":["Fine tune model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qt2eEVJ64c_5"},"outputs":[],"source":["import transformers\n","import fire\n","import torch\n","from transformers import LlamaForCausalLM, LlamaTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n","from datasets import load_dataset\n","from typing import List\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    prepare_model_for_int8_training,\n",")\n","\n","tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\", pad_token='[PAD]')\n","tokenizer.pad_token_id = 0\n","tokenizer.bos_token_id = 1\n","tokenizer.eos_token_id = 2\n","\n","def train(\n","    # model/data params\n","    output_dir: str = \"/content/gdrive/My Drive/Llama\",\n","    # training hyperparams\n","    batch_size: int = 128,\n","    micro_batch_size: int = 4,\n","    epochs: int = 2,\n","    learning_rate: float = 3e-4,\n","    #lora hyperparams\n","    lora_r: int = 8,\n","    lora_alpha: int = 16,\n","    lora_dropout: float = 0.05,\n","    lora_target_modules: List[str] = [ \"q_proj\", \"v_proj\" ]\n","  ):\n","\n","  config = LoraConfig(r=lora_r, lora_alpha=lora_alpha, target_modules=lora_target_modules, lora_dropout=lora_dropout, bias=\"none\", task_type=\"CAUSAL_LM\")\n","\n","  model = init_model(config)\n","  dataset = init_dataset(tokenizer, init_model);\n","\n","  model.print_trainable_parameters()\n","\n","  training_args = transformers.TrainingArguments(\n","      per_device_train_batch_size=micro_batch_size,\n","      gradient_accumulation_steps=batch_size // micro_batch_size,\n","      warmup_steps=100,\n","      num_train_epochs=epochs,\n","      learning_rate=learning_rate,\n","      fp16=True,\n","      optim=\"adamw_torch\",\n","      output_dir=output_dir,\n","      # save_total_limit=3,\n","      # load_best_model_at_end=True\n","      # save_steps=100,\n","      # logging_steps=10,\n","      # save_strategy=\"steps\",\n","      # do_eval=True,\n","      # evaluation_strategy=\"steps\",\n","      # eval_steps=100,\n","  )\n","\n","  trainer = transformers.Trainer(model=model,\n","                    train_dataset=dataset['train'],\n","                    eval_dataset=dataset['test'],\n","                    args=training_args)\n","  # model = torch.compile(model)\n","  trainer.train(resume_from_checkpoint=False);\n","  # trainer.evaluate()\n","  model.save_pretrained(output_dir)\n","\n","def init_model(config):\n","  model = LlamaForCausalLM.from_pretrained(\"decapoda-research/llama-7b-hf\",\n","                                        load_in_8bit=True,\n","                                        torch_dtype=torch.float16,\n","                                        device_map='auto',\n","                                        low_cpu_mem_usage=True)\n","  model = prepare_model_for_int8_training(model)\n","  model = get_peft_model(model, config)\n","  model.config.use_cache = False\n","\n","  return model\n","\n","def init_dataset(tokenizer, model):\n","  dataset = load_dataset(\"hevia/scp-embeddings\", split=\"train\")\n","  dataset = dataset.rename_column('Full Text', 'labels').remove_columns(['__index_level_0__', 'embeddings'])\n","\n","  data = dataset.train_test_split(test_size=0.1,\n","                                seed=42,\n","                                shuffle=True)\n","\n","  train_data = data['train'].map(generate_prompt, batched=True)\n","  val_data = data['test'].map(generate_prompt, batched=True)\n","\n","  train_data = train_data.remove_columns('code')\n","  val_data = val_data.remove_columns('code')\n","\n","  train_data = train_data.map(tokenize_function, batched=True)\n","  val_data = val_data.map(tokenize_function, batched=True)\n","\n","  return { 'train': train_data, 'test': val_data }\n","\n","def tokenize_function(data, max_seq_length=512, add_eos_token=True):\n","    result = tokenizer(data['labels'],\n","                       truncation=True,\n","                       padding=\"max_length\",\n","                       max_length=max_seq_length,\n","                       return_tensors=\"pt\")\n","\n","    if add_eos_token:\n","        eos_token_id = tokenizer.eos_token_id\n","        for input_ids, attention_mask in zip(result[\"input_ids\"], result[\"attention_mask\"]):\n","            if eos_token_id not in input_ids:\n","                idx = (attention_mask == 0).nonzero(as_tuple=True)[0][0]\n","                input_ids[idx] = eos_token_id\n","                attention_mask[idx] = 1\n","\n","    result[\"labels\"] = result[\"input_ids\"].clone()\n","\n","    return result\n","\n","def generate_prompt(data):\n","  data['labels'] = f\"Write description of {data['code']}\\n\\n{data['labels']}\"\n","\n","\n","\n","train()"]},{"cell_type":"markdown","metadata":{"id":"JFMUbmA142AA"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyP98gFlBJCphAHR5S7dpSYn","gpuClass":"premium","machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
